---
title: "Image Process: Super-Revolution of Low Revolution Images"
date: "2019-3-27"
author: "Guanren Wang"
output: html_notebook
---


## Project summary:  
+ In this project, we utilized two algorithms to enhance the resolution of blurry and low-resolution images. We:\
1) Implemented the current practice as the baseline model by R\
2) Implemented an improvement to the current practice\
3) Evaluated the performance gain of your proposed improvement against the baseline. We utilized SRGAN by python as improved model.For baseline model we used `doParallel`,`gbm` and `EBImage` library in R to do super-resolution of the images. We calculated the MSE and PSNR for evaluation purpose. Our model is better than the nerest-neighbor method and bilinear interpolation. Additionally, it is better than bicubic interpolation as well.
+ The MSE and PSNR for baseline model are *0.0028* and *27.4187* respectively, and the MSE and PSNR for improved model are *0.0022* (because R and Python use different scale for RGB, the best way to compare two models is to use PSNR) and *27.8961* respectively, which is obviously better than the baseline models.

This file organizes **all computational steps** for evaluating baseline image processing framework.\
All major steps of my computation are computed by **self-defined functions**. If you want to see code of these functions, please click [here](https://github.com/Grandeurwang/Super_Revolution_of_low_revoltion_images/tree/master/lib)\
For computational steps of improved model, please click [here](https://github.com/Grandeurwang/Super_Revolution_of_low_revoltion_images/tree/master/doc)\
All the code is available on [GitHub](https://github.com/Grandeurwang/Super_Revolution_of_low_revoltion_images)


The baseline model(GBM) is computed via Google Cloud VM with machine type: **n1-standard-16 (16 vCPUs, 60 GB memory)** and CPU platform: **Intel Broadwell**.
In view of heavy computing workload, I recommend you use VM to reproduce.

### Load libraries
```{r load library,warning=FALSE}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
  library("EBImage")
}

if(!require("gbm")){
  install.packages("gbm")
  library("gbm")
}

library("EBImage")
library("gbm")
```

### Parallel Computing Setup
```{r ParallelSetup}
if(!require("doParallel")){
  install.packages("doParallel")
  library("doParallel")
}

# Real physical cores in the computer
cores <- detectCores()

if(cores>1){
  
  if(.Platform$OS.type=="windows"){
    cl <- makeCluster(cores)
    registerDoParallel(cl, cores=cores)
  }
  else{
    if(!require("doMC")){
      install.packages("doMC")
      library("doMC")
    }
    registerDoMC(cores)
  }
  
  run.parallel=TRUE

}else
  run.parallel=FALSE
cores<-cores
```

## Step 0: Specify Directories.
Provide directories for training images. **Low-resolution (LR)** image set and **High-resolution (HR)** image set will be in different subfolders. 
```{r wkdir}
set.seed(123)
# use relative path for reproducibility

train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_LR_dir <- paste(train_dir, "LR/", sep="")
train_HR_dir <- paste(train_dir, "HR/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 
```

## Step 1: Set Up Controls For Evaluation Experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) whether run training
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
run.cv=T # run cross-validation on the training set
K <- 5  # number of CV folds
run.train=TRUE # whether run training
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set

```

Using cross-validation or independent test set evaluation, we compare the performance of models with different specifications. In this project, we use GBM with different `depth`. In the following chunk, we list, in a vector, setups (in this case, `depth`) corresponding to models that we will compare. In your project, you might compare very different classifiers. You can assign them numerical IDs and labels specific to your project. 

```{r model_setup}
model_values <- 2:6
model_labels = paste("GBM with depth =", model_values)
```

## Step 2: Import Training Images Class Labels.

We provide extra information of image label: car (0), flower (1), market (2). These labels are not necessary for your model.

```{r train_label}
extra_label <- read.csv(train_label_path, colClasses=c("NULL", NA, NA))
```

## Step 3: Construct Features and Responses

`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. 
  
`feature.R`:

  + Input: a path for low-resolution images.
  + Input: a path for high-resolution images.
  + Output: an RData file that contains extracted features and corresponding responses

I do recommend using non-parallel version here, as extract feature is a data-intensive --- not computation intensive step. Thus, parallel computing won't significantly save time --- even increase running time, for the parallel computing will raise additional coordination cost, depends on different hardwares: 

| Running Time | HPC | Pixelbook | old laptop |
|--------------|:---:|:---------:|:----------:|
|   Parallel   | 19  | 55        | 80         |
| Non-Parallel | 51  | 33        | 90         |

```{r run parallel or not}
if(run.parallel)
  source("../lib/feature_parallel.R") else
    source("../lib/feature.R")
```

```{r feature}
tm_feature_train <- NA
if(run.feature.train){
  tm0=proc.time()
  dat_train <- feature(train_LR_dir, train_HR_dir)
  tm_feature_train=proc.time()-tm0
  
  save(dat_train, file="../output/feature_train.RData")
}else{
  load("../output/feature_train.RData")
}
feat_train <- dat_train$feature
label_train <- dat_train$label
rm(dat_train)
tmp=gc() # release memory
```


## Step 4: Train a classification model with training images
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 

`train.R`:

  + Input: a path that points to the training set features and responses.
  + Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
  
`test.R`:

  + Input: a path that points to the test set features.
  + Input: an R object that contains a trained classifier.
  + Output: an R object of response predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib}
if(run.parallel)
  source("../lib/train_parallel.R") else
    source("../lib/train.R")

### The test parallel will increase much more time, so we use non-paralleled one
source("../lib/test.R")
```

### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth for GBM in this example. 
```{r runcv, message=FALSE, warning=FALSE}
source("../lib/cross_validation.R")
if(run.cv){
  err_cv <- array(dim=c(length(model_values), 2))
  for(k in 1:length(model_values)){
    cat("k=", k, "\n")
    err_cv[k,] <- cv.function(feat_train, label_train, model_values[k], K)
    system("free -m")
  }
  save(err_cv, file="../output/err_cv.RData")
}else{
  load("../output/err_cv.RData")
}
tmp=gc() # release memory
```

### Visualize cross-validation results
```{r cv_vis}
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
       main="Cross Validation Error", type="n", ylim=c(0, 0.01))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2], 
        length=0.1, angle=90, code=3)
psnr<-10*log10(1/err_cv[,1])
plot(model_values, psnr, xlab="Interaction Depth", ylab="PSNR",
       main="PSNR", type="n")
points(model_values, psnr, col="red", pch=16)
lines(model_values, psnr, col="red")
```


### Choose the "best" parameter value
```{r best_model}
model_best=model_values[1]

model_best <- model_values[which.min(err_cv[,1])]
cat("The min cv is",min(err_cv[,1]),"for depth",model_best,"\n")

par_best <- list(depth=model_best)
```

### Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train}
source("../lib/train_parallel.R")
if(run.train){
  tm0=proc.time()
  fit_train <- train(feat_train, label_train, par_best)
  tm_train=proc.time()-tm0
  save(fit_train, file="../output/fit_train.RData")
}else{
  load("../output/fit_train.RData")
}
tmp=gc()
```

## Step 5: Super-resolution for test images
Feed the final training model with the completely holdout testing data. 

`superResolution.R`:

  + Input: a path that points to the folder of low-resolution test images.
  + Input: a path that points to the folder (empty) of high-resolution test images.
  + Input: an R object model with tuned parameters.
  + Output: construct high-resolution versions for each low-resolution test image.
```{r superresolution}
# useually we use parallel computing to generate new images, but because of incompatibility of operating systems, superrevolution doesn't work, in this case we could do this without parallel computing. 
# for non-parallel computing, just delete pound sign below

# run.parallel=F
if(run.parallel)
  source("../lib/superResolution_parallel.R") else
    source("../lib/superResolution.R")
test_dir <- "../data/test_set/" # This will be modified for different data sets.
test_LR_dir <- paste(test_dir, "LR/", sep="")
test_HR_dir <- paste(test_dir, "HR/", sep="")

if(run.test){
  tm0=proc.time()
  superResolution(test_LR_dir, test_HR_dir, fit_train)
  tm_test=proc.time()-tm0
}
run.parallel=T
```

## Step 6: Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for constructing training features=", tm_feature_train[3], "s \n")
cat("Time for training model=", tm_train[3], "s \n")
cat("Time for super-resolution=", tm_test[3], "s \n")
```

## Step 7: Calculate MSE and PSNR
```{r accuracy}
train_dir <- "../data/train_set/HR/"
test_dir <- "../data/test_set/HR/"
source("../lib/Evaluation.R")
mp=msepsnr(train_dir, test_dir)
cat("MSE is", mp[1],'\n')

cat("PSNR is", mp[2])
```

## Step 8: Stop Parallel Computing 
```{r stopParallel}
if(run.parallel & .Platform$OS.type=="windows"){
  stopImplicitCluster()
  stopCluster(cl)
}
```


## Conclusion:

### Steps:

Our baseline model consists of four **steps**:\
1. Preprocessing and Feature Extraction\
2. Model training, evaluation(parameter tuning) and selection\
3. Making prediction using selected model\
4. Evaluating prediction and running time of model

### Results comparison:

We ran feature extraction by sampling 1000 points from the LR image pixels and takin the 8 neighboring pixels for each point as features. 

Optimal depth is 6 with cross-validation error 0.00266912

Time for constructing training features= 92.81 s
Time for training model is around 3 hours 
Time for super-resolution= 2717.55 s (1500 images)

**MSE** is 0.002669120\
**PSNR** is 25.73632

It is better than the **nerest-neighbor method** which uses the value of nearby translated pixel values for the output pixel values. MSE of this method is 0.004035 and PSNR is 25.3926.

It is also better than **bilinear interpolation** which uses the weighted average of two translated pixel values for each output pixel value. The MSE is 0.003277 and PSNR is 26.3723.

Additionally, it is better than **bicubic interpolation** as well, which uses the weighted average of four translated pixel values for each output pixel value. MSE is 0.004927 and the PSNR is 24.1553. 